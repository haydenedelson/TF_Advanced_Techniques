{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copy of C3W2_Assignment.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOvvWAVTkMR7"
      },
      "source": [
        "# Week 2 Assignment: Zombie Detection\n",
        "\n",
        "Welcome to this week's programming assignment! You will use the Object Detection API and retrain [RetinaNet](https://arxiv.org/abs/1708.02002) to spot Zombies using just 5 training images. You will setup the model to restore pretrained weights and fine tune the classification layers.\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=18Ck0qNSZy9F1KsUKWc4Jv7_x_1e_fXTN' alt='zombie'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GKykVXq8elD"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "* [Exercise 1 - Import Object Detection API packages](#exercise-1)\n",
        "* [Exercise 2 - Visualize the training images](#exercise-2)\n",
        "* [Exercise 3 - Define the category index dictionary](#exercise-3)\n",
        "* [Exercise 4 - Download checkpoints](#exercise-4)\n",
        "* [Exercise 5.1 - Locate and read from the configuration file](#exercise-5-1)\n",
        "* [Exercise 5.2 - Modify the model configuration](#exercise-5-2)\n",
        "* [Exercise 5.3 - Modify model_config](#exercise-5-3)\n",
        "* [Exercise 5.4 - Build the custom model](#exercise-5-4)\n",
        "* [Exercise 6.1 - Define Checkpoints for the box predictor](#exercise-6-1)\n",
        "* [Exercise 6.2 - Define the temporary model checkpoint](#exercise-6-2)\n",
        "* [Exercise 6.3 - Restore the checkpoint](#exercise-6-2)\n",
        "* [Exercise 7 - Run a dummy image to generate the model variables](#exercise-7)\n",
        "* [Exercise 8 - Set training hyperparameters](#exercise-8)\n",
        "* [Exercise 9 - Select the prediction layer variables](#exercise-9)\n",
        "* [Exercise 10 - Define the training step](#exercise-10)\n",
        "* [Exercise 11 - Preprocess, predict, and post process an image](#exercise-11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPs64QA1Zdov"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR-J7whqrkIl"
      },
      "source": [
        "You'll start by installing the Tensorflow 2 [Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi28cqGGFWnY"
      },
      "source": [
        "# Delete an existing models directory\n",
        "!rm -rf ./models/\n",
        "\n",
        "# Clone the Tensorflow Model Garden\n",
        "!git clone --depth 1 https://github.com/tensorflow/models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwdsBdGhFanc"
      },
      "source": [
        "# Install Object Detection API\n",
        "!cd models/research/ && protoc object_detection/protos/*.proto --python_out=. && cp object_detection/packages/tf2/setup.py . && python -m pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21tUtyyVrkIt"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZcqD4NLdnf4"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "\n",
        "import glob\n",
        "import imageio\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from IPython.display import display, Javascript\n",
        "from IPython.display import Image as IPyImage\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-auwvBQrkIw"
      },
      "source": [
        "<a name='exercise-1'></a>\n",
        "### **Exercise 1**: Import Object Detection API packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YihOFxxrkIw"
      },
      "source": [
        "# Import object detection utilities\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.builders import model_builder\n",
        "from object_detection.utils import colab_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IogyryF2lFBL"
      },
      "source": [
        "## Utilities\n",
        "\n",
        "You'll define a couple of utility functions for loading images and plotting detections. This code is provided for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y9R0Xllefec"
      },
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "    \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "    Puts image into numpy array to feed into tensorflow graph.\n",
        "    Note that by convention we put it into a numpy array with shape\n",
        "    (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "    Args:\n",
        "    path: a file path.\n",
        "\n",
        "    Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    \"\"\"\n",
        "    \n",
        "    img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "    image = Image.open(BytesIO(img_data))\n",
        "    (im_width, im_height) = image.size\n",
        "    \n",
        "    return np.array(image.getdata()).reshape(\n",
        "        (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "\n",
        "def plot_detections(image_np,\n",
        "                    boxes,\n",
        "                    classes,\n",
        "                    scores,\n",
        "                    category_index,\n",
        "                    figsize=(12, 16),\n",
        "                    image_name=None):\n",
        "    \"\"\"Wrapper function to visualize detections.\n",
        "\n",
        "    Args:\n",
        "    image_np: uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    boxes: a numpy array of shape [N, 4]\n",
        "    classes: a numpy array of shape [N]. Note that class indices are 1-based,\n",
        "          and match the keys in the label map.\n",
        "    scores: a numpy array of shape [N] or None.  If scores=None, then\n",
        "          this function assumes that the boxes to be plotted are groundtruth\n",
        "          boxes and plot all boxes as black with no classes or scores.\n",
        "    category_index: a dict containing category dictionaries (each holding\n",
        "          category index `id` and category name `name`) keyed by category indices.\n",
        "    figsize: size for the figure.\n",
        "    image_name: a name for the image file.\n",
        "    \"\"\"\n",
        "    \n",
        "    image_np_with_annotations = image_np.copy()\n",
        "    \n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np_with_annotations,\n",
        "        boxes,\n",
        "        classes,\n",
        "        scores,\n",
        "        category_index,\n",
        "        use_normalized_coordinates=True,\n",
        "        min_score_thresh=0.8)\n",
        "    \n",
        "    if image_name:\n",
        "        plt.imsave(image_name, image_np_with_annotations)\n",
        "    \n",
        "    else:\n",
        "        plt.imshow(image_np_with_annotations)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSaXL28TZfk1"
      },
      "source": [
        "## Download the Zombie data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqSFoJz2Cgzs"
      },
      "source": [
        "# Delete existing zip and training directory\n",
        "!rm training-zombie.zip\n",
        "!rm -rf ./training\n",
        "\n",
        "# Download zombie images\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training-zombie.zip \\\n",
        "    -O ./training-zombie.zip\n",
        "\n",
        "# Unzip\n",
        "local_zip = './training-zombie.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./training')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyzSGUDsrkI7"
      },
      "source": [
        "<a name='exercise-2'></a>\n",
        "\n",
        "### **Exercise 2**: Visualize the training images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQy3ND7EpFQM"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Set directory name\n",
        "train_image_dir = './training'\n",
        "\n",
        "# Initialize list to hold image data\n",
        "train_images_np = []\n",
        "\n",
        "# Iterate over training images\n",
        "for i in range(1, 6):\n",
        "\n",
        "    image_path = os.path.join(train_image_dir, 'training-zombie' + str(i) + '.jpg')\n",
        "    print(image_path)\n",
        "\n",
        "    # Load images into numpy arrays and append to a list\n",
        "    train_images_np.append(load_image_into_numpy_array(image_path))\n",
        "\n",
        "# Configure plot settings\n",
        "plt.rcParams['axes.grid'] = False\n",
        "plt.rcParams['xtick.labelsize'] = False\n",
        "plt.rcParams['ytick.labelsize'] = False\n",
        "plt.rcParams['xtick.top'] = False\n",
        "plt.rcParams['xtick.bottom'] = False\n",
        "plt.rcParams['ytick.left'] = False\n",
        "plt.rcParams['ytick.right'] = False\n",
        "plt.rcParams['figure.figsize'] = [14, 7]\n",
        "\n",
        "# Plot images\n",
        "for idx, train_image_np in enumerate(train_images_np):\n",
        "    plt.subplot(1, 5, idx+1)\n",
        "    plt.imshow(train_image_np)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqb_yjAo3cO_"
      },
      "source": [
        "<a name='gt_boxes_definition'></a>\n",
        "## Prepare data for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqDVC6epheJZ"
      },
      "source": [
        "# Define the list of ground truth boxes\n",
        "gt_boxes = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nEDRoUEcUgL"
      },
      "source": [
        "# Draw ground truth boxes\n",
        "colab_utils.annotate(train_images_np, box_storage_pointer=gt_boxes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsSVYhTVZZSP"
      },
      "source": [
        "# TEST CODE:\n",
        "try:\n",
        "  assert(len(gt_boxes) == 5), \"Warning: gt_boxes is empty. Did you click `submit`?\"\n",
        "\n",
        "except AssertionError as e:\n",
        "  print(e)\n",
        "\n",
        "# checks if there are boxes for all 5 images\n",
        "for gt_box in gt_boxes:\n",
        "    try:\n",
        "      assert(gt_box is not None), \"There are less than 5 sets of box coordinates. \" \\\n",
        "                                  \"Please re-run the cell above to draw the boxes again.\\n\" \\\n",
        "                                  \"Alternatively, you can run the next cell to load pre-determined \" \\\n",
        "                                  \"ground truth boxes.\"\n",
        "    \n",
        "    except AssertionError as e:\n",
        "        print(e)\n",
        "        break\n",
        "\n",
        "\n",
        "ref_gt_boxes = [\n",
        "        np.array([[0.27333333, 0.41500586, 0.74333333, 0.57678781]]),\n",
        "        np.array([[0.29833333, 0.45955451, 0.75666667, 0.61078546]]),\n",
        "        np.array([[0.40833333, 0.18288394, 0.945, 0.34818288]]),\n",
        "        np.array([[0.16166667, 0.61899179, 0.8, 0.91910903]]),\n",
        "        np.array([[0.28833333, 0.12543962, 0.835, 0.35052755]]),\n",
        "      ]\n",
        "\n",
        "for gt_box, ref_gt_box in zip(gt_boxes, ref_gt_boxes):\n",
        "    try:\n",
        "      assert(np.allclose(gt_box, ref_gt_box, atol=0.04)), \"One of the boxes is too big or too small. \" \\\n",
        "                                                          \"Please re-draw and make the box tighter around the zombie.\"\n",
        "    \n",
        "    except AssertionError as e:\n",
        "      print(e)\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olhzhCH5heJa"
      },
      "source": [
        "#### View your ground truth box coordinates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxd66yq-M9Va"
      },
      "source": [
        "# Print ground truth coordinates\n",
        "for gt_box in gt_boxes:\n",
        "  print(gt_box)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVeDQaSdrkJC"
      },
      "source": [
        "<a name='exercise-3'></a>\n",
        "\n",
        "### **Exercise 3**: Define the category index dictionary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWBqFVMcweF-"
      },
      "source": [
        "# Set zombie class ID\n",
        "zombie_class_id = 1\n",
        "\n",
        "# Define zombie class dictionary\n",
        "category_index = {zombie_class_id: {'id': zombie_class_id, 'name': 'zombie'}}\n",
        "\n",
        "# Specify number of classes\n",
        "num_classes = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTvj7Q0oYk75"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H68ot6BkrkJH"
      },
      "source": [
        "# Set label ID offset\n",
        "label_id_offset = 1\n",
        "\n",
        "# Initialize list for image tensors\n",
        "train_image_tensors = []\n",
        "\n",
        "# Initialize lists for ground truth classes and bounding boxes\n",
        "gt_classes_one_hot_tensors = []\n",
        "gt_box_tensors = []\n",
        "\n",
        "for (train_image_np, gt_box_np) in zip(train_images_np, gt_boxes):\n",
        "    \n",
        "    # Convert training image to tensor and add to list\n",
        "    train_image_tensors.append(tf.expand_dims(tf.convert_to_tensor(\n",
        "        train_image_np, dtype=tf.float32), axis=0))\n",
        "    \n",
        "    # Convert GT box array to tensor and add to list\n",
        "    gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))\n",
        "    \n",
        "    # Apply offset to get zero-indexed GT classes\n",
        "    zero_indexed_gt_classes = tf.convert_to_tensor(\n",
        "        np.ones(shape=[gt_box_np.shape[0]], dtype=np.int32) - label_id_offset)\n",
        "    \n",
        "    # Get one-hot encoded gt classes\n",
        "    gt_classes_one_hot_tensors.append(tf.one_hot(\n",
        "        zero_indexed_gt_classes, num_classes))\n",
        "\n",
        "print('Done prepping data.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3_Z3mJWN9KJ"
      },
      "source": [
        "## Visualize the zombies with their ground truth bounding boxes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBD6l-E4N71y"
      },
      "source": [
        "# Give boxes a score of 100%\n",
        "dummy_scores = np.array([1.0], dtype=np.float32)\n",
        "\n",
        "# Plot images\n",
        "plt.figure(figsize=(30, 15))\n",
        "for idx in range(5):\n",
        "    plt.subplot(2, 4, idx+1)\n",
        "    plot_detections(\n",
        "      train_images_np[idx],\n",
        "      gt_boxes[idx],\n",
        "      np.ones(shape=[gt_boxes[idx].shape[0]], dtype=np.int32),\n",
        "      dummy_scores, category_index)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghDAsqfoZvPh"
      },
      "source": [
        "## Download the checkpoint containing the pre-trained weights\n",
        "\n",
        "Next, you will download [RetinaNet](https://arxiv.org/abs/1708.02002) and copy it inside the object detection directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn21-EelheJa"
      },
      "source": [
        "<a name='exercise-4'></a>\n",
        "### Exercise 4: Download checkpoints\n",
        "\n",
        "  - Download the compressed SSD Resnet 50 version 1, 640 x 640 checkpoint.\n",
        "  - Untar (decompress) the tar file\n",
        "  - Move the decompressed checkpoint to `models/research/object_detection/test_data/`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J16r3NChD-7"
      },
      "source": [
        "# Download the SSD Resnet 50 version 1, 640x640 checkpoint\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "    \n",
        "# Untar file\n",
        "!tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "\n",
        "# Copy checkpoint to test_data folder\n",
        "!mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgG_YT7UrkJQ"
      },
      "source": [
        "## Configure the model\n",
        "Here, you will configure the model for this use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3eHyNmxheJa"
      },
      "source": [
        "<a name='exercise-5-1'></a>\n",
        "\n",
        "### **Exercise 5.1**: Locate and read from the configuration file\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59sEM6wXheJa"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Define .config file path\n",
        "pipeline_config = 'models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
        "\n",
        "# Load config file\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
        "\n",
        "# Show configs\n",
        "configs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAkgGnMcheJa"
      },
      "source": [
        "<a name='exercise-5-2'></a>\n",
        "\n",
        "### **Exercise 5.2**: Get the model configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps3AzqBvheJa"
      },
      "source": [
        "# Get 'model' configs object\n",
        "model_config = configs['model']\n",
        "\n",
        "# Show model_config\n",
        "model_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxHqQK1eheJa"
      },
      "source": [
        "<a name='exercise-5-3'></a>\n",
        "\n",
        "### **Exercise 5.3**: Modify model_config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyT4BUbaMeG-"
      },
      "source": [
        "# Modify num_classes\n",
        "model_config.ssd.num_classes = num_classes\n",
        "\n",
        "# Freeze batch normalization\n",
        "model_config.ssd.freeze_batchnorm = True\n",
        "\n",
        "# Show updated model_config\n",
        "model_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFnuT2rfheJa"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvtrqsJIrkJT"
      },
      "source": [
        "<a name='exercise-5.4'></a>\n",
        "\n",
        "### **Exercise 5.4**: Build the custom model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfoRTeV_rkJT"
      },
      "source": [
        "# Build new model using model_config\n",
        "detection_model = model_builder.build(model_config, is_training=True, add_summaries=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C97Zo0OJnOf"
      },
      "source": [
        "## Restore weights from your checkpoint\n",
        "\n",
        "Now, you will selectively restore weights from your checkpoint.\n",
        "- The parts of RetinaNet that you want to reuse are:\n",
        "  - Feature extraction layers\n",
        "  - Bounding box regression prediction layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BK06DyFheJa"
      },
      "source": [
        "<a name='exercise-6-1'></a>\n",
        "### Exercise 6.1: Define Checkpoints for the box predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KPWWYbyrkJY"
      },
      "source": [
        "# Define checkpoint to load box predictor\n",
        "tmp_box_predictor_checkpoint = tf.train.Checkpoint(\n",
        "    _base_tower_layers_for_heads = detection_model._box_predictor._base_tower_layers_for_heads,\n",
        "    _box_prediction_head = detection_model._box_predictor._box_prediction_head\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqMyPTU1rkJa"
      },
      "source": [
        "<a name='exercise-6-2'></a>\n",
        "### Exercise 6.2: Define the temporary model checkpoint**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCqGfrawrkJa"
      },
      "source": [
        "# Define checkpoint to load temporary model\n",
        "tmp_model_checkpoint = tf.train.Checkpoint(\n",
        "    _feature_extractor = detection_model._feature_extractor,\n",
        "    _box_predictor = tmp_box_predictor_checkpoint\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_BtL4ZZKVvV"
      },
      "source": [
        "<a name='exercise-6-3'></a>\n",
        "### Exercise 6.3: Restore the checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elkvDUUarkJg"
      },
      "source": [
        "checkpoint_path = 'models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
        "\n",
        "# Define checkpoint to restore model\n",
        "checkpoint = tf.train.Checkpoint(model=tmp_model_checkpoint)\n",
        "\n",
        "# Restore checkpoint to checkpoint_path\n",
        "checkpoint.restore(checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k6oFoxTrkJi"
      },
      "source": [
        "<a name='exercise-7'></a>\n",
        "### **Exercise 7**: Run a dummy image to generate the model variables\n",
        "\n",
        "Run a dummy image through the model so that variables are created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MANTSP9LrkJi"
      },
      "source": [
        "# Run dummy image through model to restore weights\n",
        "tmp_image, tmp_shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
        "tmp_prediction_dict = detection_model.predict(tmp_image, tmp_shapes)\n",
        "tmp_detections = detection_model.postprocess(tmp_prediction_dict, tmp_shapes)\n",
        "\n",
        "print('Weights restored!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCkWmdoZZ0zJ"
      },
      "source": [
        "## Eager mode custom training loop\n",
        "\n",
        "With the data and model now setup, you can now proceed to configure the training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njlV4PoPrkJq"
      },
      "source": [
        "<a name='exercise-8'></a>\n",
        "### **Exercise 8**: Set training hyperparameters\n",
        "\n",
        "Set an appropriate learning rate and optimizer for the training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyHoF4mUrv5-"
      },
      "source": [
        "# Set model training hyperparameters\n",
        "tf.keras.backend.set_learning_phase(True)\n",
        "\n",
        "# Batch size\n",
        "batch_size = 4\n",
        "\n",
        "# Number of batches\n",
        "num_batches = 1000\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-MaJmHmrkJs"
      },
      "source": [
        "## Choose the layers to fine-tune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91r2dk-7heJb"
      },
      "source": [
        "# Inspect the layers of detection_model\n",
        "for i, v in enumerate(detection_model.trainable_variables):\n",
        "    print(\"i: {} \\t name: {} \\t shape:{} \\t dtype={}\".format(i, v.name, v.shape, v.dtype))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6S9QjSWheJb"
      },
      "source": [
        "<a name='exercise-9'></a>\n",
        "\n",
        "### **Exercise 9**: Select the prediction layer variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDdvGRCYrkJt"
      },
      "source": [
        "# Collect layers to fine tune\n",
        "to_fine_tune = []\n",
        "prefixes_to_train = [\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead',\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead'\n",
        "]\n",
        "\n",
        "for var in detection_model.trainable_variables:\n",
        "  if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n",
        "    to_fine_tune.append(var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffvhZZu2rkJy"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIUKsYPLheJb"
      },
      "source": [
        "<a name='exercise-10'></a>\n",
        "\n",
        "### **Exercise 10**: Define the training step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "nttIf_ZgheJb"
      },
      "source": [
        "# Define training step\n",
        "@tf.function\n",
        "def train_step_fn(image_list,\n",
        "                groundtruth_boxes_list,\n",
        "                groundtruth_classes_list,\n",
        "                model,\n",
        "                optimizer,\n",
        "                vars_to_fine_tune):\n",
        "    \"\"\"A single training iteration.\n",
        "\n",
        "    Args:\n",
        "      image_list: A list of [1, height, width, 3] Tensor of type tf.float32.\n",
        "        Note that the height and width can vary across images, as they are\n",
        "        reshaped within this function to be 640x640.\n",
        "      groundtruth_boxes_list: A list of Tensors of shape [N_i, 4] with type\n",
        "        tf.float32 representing groundtruth boxes for each image in the batch.\n",
        "      groundtruth_classes_list: A list of Tensors of shape [N_i, num_classes]\n",
        "        with type tf.float32 representing groundtruth boxes for each image in\n",
        "        the batch.\n",
        "\n",
        "    Returns:\n",
        "      A scalar tensor representing the total loss for the input batch.\n",
        "    \"\"\"\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        # Preprocess input images\n",
        "        preprocessed_images = []\n",
        "        image_shapes = []\n",
        "\n",
        "        for img in image_list:\n",
        "          preprocessed_img, img_shape = model.preprocess(img)\n",
        "          preprocessed_images.append(preprocessed_img)\n",
        "          image_shapes.append(img_shape)\n",
        "\n",
        "        preprocessed_images_tensor = tf.concat(preprocessed_images, axis=0)\n",
        "        image_shapes_tensor = tf.concat(image_shapes, axis=0)\n",
        "\n",
        "        # Make a prediction\n",
        "        prediction_dict = model.predict(preprocessed_images_tensor, image_shapes_tensor)\n",
        "\n",
        "        # Calculate loss\n",
        "        model.provide_groundtruth(\n",
        "            groundtruth_boxes_list=groundtruth_boxes_list,\n",
        "            groundtruth_classes_list=groundtruth_classes_list)\n",
        "        \n",
        "        losses_dict = model.loss(prediction_dict, image_shapes_tensor)\n",
        "        total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
        "\n",
        "        # Calculate the gradients\n",
        "        gradients = tape.gradient(total_loss, vars_to_fine_tune)\n",
        "\n",
        "        # Optimize training variables\n",
        "        optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
        "        \n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mozyy8ns6Rx2"
      },
      "source": [
        "# Define early stopping callback\n",
        "def early_stop_callback(losses, min_delta=0.00001, patience=100):\n",
        "  # No early stopping for at least (2 * patience) epochs \n",
        "  if len(losses) < 2 * patience:\n",
        "    return False\n",
        "  \n",
        "  # Get recent losses\n",
        "  recent_losses = np.array(losses[-(patience + 1):])\n",
        "\n",
        "  # Pop current loss\n",
        "  current_loss, recent_losses = recent_losses[-1], recent_losses[:-1]\n",
        "\n",
        "  # Test whether current_loss is less than moving avg loss\n",
        "  if current_loss < (np.mean(recent_losses, axis=0) - min_delta):\n",
        "    return False\n",
        "  else:\n",
        "    return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n7imaJRrkJ1"
      },
      "source": [
        "## Run the training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgpDQ3JbrkJ3"
      },
      "source": [
        "print('Start fine-tuning!', flush=True)\n",
        "\n",
        "# Define list to store losses\n",
        "losses = []\n",
        "\n",
        "for idx in range(num_batches):\n",
        "  # Grab indices for random sample of images\n",
        "  indices = list(range(len(train_images_np)))\n",
        "  random.shuffle(indices)\n",
        "  sample_keys = indices[:batch_size]\n",
        "\n",
        "  # Get ground truth classes & boxes\n",
        "  gt_classes_list = [gt_classes_one_hot_tensors[key] for key in sample_keys]\n",
        "  gt_boxes_list = [gt_box_tensors[key] for key in sample_keys]\n",
        "  \n",
        "  # Get training images\n",
        "  image_tensors = [train_image_tensors[key] for key in sample_keys]\n",
        "\n",
        "  # Training step\n",
        "  total_loss = train_step_fn(image_tensors, \n",
        "                              gt_boxes_list, \n",
        "                              gt_classes_list,\n",
        "                              detection_model,\n",
        "                              optimizer,\n",
        "                              to_fine_tune\n",
        "                            )\n",
        "  \n",
        "  # Save loss\n",
        "  losses.append(total_loss)\n",
        "\n",
        "  # Print progress\n",
        "  if idx % 10 == 0:\n",
        "    print('batch ' + str(idx) + ' of ' + str(num_batches)\n",
        "    + ', loss=' +  str(total_loss.numpy()), flush=True)\n",
        "  \n",
        "  # Test for early stop\n",
        "  if early_stop_callback(losses):\n",
        "    # Print final output\n",
        "    print('\\nStopped at batch ' + str(idx) + ' of ' + str(num_batches)\n",
        "    + ', loss=' +  str(total_loss.numpy()), flush=True)\n",
        "    break\n",
        "\n",
        "print('Done fine-tuning!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHlXL1x_Z3tc"
      },
      "source": [
        "## Load test images and run inference with new model!\n",
        "\n",
        "You can now test your model on a new set of images. The cell below downloads 237 images of a walking zombie and stores them in a `results/` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL697_MnHcJF"
      },
      "source": [
        "# Delete existing files\n",
        "!rm zombie-walk-frames.zip\n",
        "!rm -rf ./zombie-walk\n",
        "!rm -rf ./results\n",
        "\n",
        "# Download test images\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/zombie-walk-frames.zip \\\n",
        "    -O zombie-walk-frames.zip\n",
        "\n",
        "# Unzip test images\n",
        "local_zip = './zombie-walk-frames.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./results')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_LJJ8wWhzlZ"
      },
      "source": [
        "Load these images into a numpy array to prepare for inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcE6OwrHQJya"
      },
      "source": [
        "test_image_dir = './results/'\n",
        "test_images_np = []\n",
        "\n",
        "# Load images into a numpy array\n",
        "for i in range(0, 237):\n",
        "    image_path = os.path.join(test_image_dir, 'zombie-walk' + \"{0:04}\".format(i) + '.jpg')\n",
        "    print(image_path)\n",
        "    test_images_np.append(np.expand_dims(\n",
        "      load_image_into_numpy_array(image_path), axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-GIbqkzrkJ_"
      },
      "source": [
        "<a name='exercise-11'></a>\n",
        "\n",
        "### **Exercise 11**: Preprocess, predict, and post process an image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aXt-bodrkKA"
      },
      "source": [
        "@tf.function\n",
        "def detect(input_tensor):\n",
        "    \"\"\"Run detection on an input image.\n",
        "\n",
        "    Args:\n",
        "    input_tensor: A [1, height, width, 3] Tensor of type tf.float32.\n",
        "      Note that height and width can be anything since the image will be\n",
        "      immediately resized according to the needs of the model within this\n",
        "      function.\n",
        "\n",
        "    Returns:\n",
        "    A dict containing 3 Tensors (`detection_boxes`, `detection_classes`,\n",
        "      and `detection_scores`).\n",
        "    \"\"\"\n",
        "    preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
        "    prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
        "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
        "    \n",
        "    return detections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhXwX7O8rkKC"
      },
      "source": [
        "# Set label ID offset\n",
        "label_id_offset = 1\n",
        "\n",
        "# Initialize dict to store results\n",
        "results = {'boxes': [], 'scores': []}\n",
        "\n",
        "# Iterate over test images to get results\n",
        "for i in range(len(test_images_np)):\n",
        "\n",
        "    # Convert input image array to tensor\n",
        "    input_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)\n",
        "\n",
        "    # Detect on input image\n",
        "    detections = detect(input_tensor)\n",
        "\n",
        "    # Plot detections\n",
        "    plot_detections(\n",
        "      test_images_np[i][0],\n",
        "      detections['detection_boxes'][0].numpy(),\n",
        "      detections['detection_classes'][0].numpy().astype(np.uint32)\n",
        "      + label_id_offset,\n",
        "      detections['detection_scores'][0].numpy(),\n",
        "      category_index, figsize=(15, 20), image_name=\"./results/gif_frame_\" + ('%03d' % i) + \".jpg\")\n",
        "    \n",
        "    # Save results\n",
        "    results['boxes'].append(detections['detection_boxes'][0][0].numpy())\n",
        "    results['scores'].append(detections['detection_scores'][0][0].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNV5uwN7rkKE"
      },
      "source": [
        "# TEST CODE\n",
        "\n",
        "# Compare with expected bounding boxes\n",
        "print(np.allclose(results['boxes'][0], [0.28838485, 0.06830047, 0.7213766 , 0.19833465], rtol=0.18))\n",
        "print(np.allclose(results['boxes'][5], [0.29168868, 0.07529271, 0.72504973, 0.20099735], rtol=0.18))\n",
        "print(np.allclose(results['boxes'][10], [0.29548776, 0.07994056, 0.7238164 , 0.20778716], rtol=0.18))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDEYAkg3l4lh"
      },
      "source": [
        "# Check percent of frames where a zombie is detected\n",
        "scores = np.array(results['scores'])\n",
        "zombie_detected = (np.where(scores > 0.9, 1, 0).sum())/237*100\n",
        "print(zombie_detected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww0hLDI4fB1f"
      },
      "source": [
        "# Inspect individual frames\n",
        "\n",
        "print('Frame 0')\n",
        "display(IPyImage('./results/gif_frame_000.jpg'))\n",
        "print()\n",
        "print('Frame 5')\n",
        "display(IPyImage('./results/gif_frame_005.jpg'))\n",
        "print()\n",
        "print('Frame 10')\n",
        "display(IPyImage('./results/gif_frame_010.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrjHwbkVXqLG"
      },
      "source": [
        "## Create a zip of the zombie-walk images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeKSNOG7BVd4"
      },
      "source": [
        "# Create zombie detections zip file\n",
        "\n",
        "zipf = zipfile.ZipFile('./zombie.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "\n",
        "filenames = glob.glob('./results/gif_frame_*.jpg')\n",
        "filenames = sorted(filenames)\n",
        "\n",
        "for filename in filenames:\n",
        "    zipf.write(filename)\n",
        "\n",
        "zipf.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTDH45KYX1Iv"
      },
      "source": [
        "## Create Zombie animation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW1FrT2iNnpy"
      },
      "source": [
        "# Create zombie detections GIF\n",
        "\n",
        "imageio.plugins.freeimage.download()\n",
        "\n",
        "!rm -rf ./results/zombie-anim.gif\n",
        "\n",
        "anim_file = './zombie-anim.gif'\n",
        "\n",
        "filenames = glob.glob('./results/gif_frame_*.jpg')\n",
        "filenames = sorted(filenames)\n",
        "last = -1\n",
        "images = []\n",
        "\n",
        "for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    images.append(image)\n",
        "\n",
        "imageio.mimsave(anim_file, images, 'GIF-FI', fps=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdRXjK-xhTDA"
      },
      "source": [
        "## Save results file for grading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0PbSjPXrkKN"
      },
      "source": [
        "import pickle\n",
        "\n",
        "# Remove file if it exists\n",
        "!rm results.data\n",
        "\n",
        "# Write results to binary file\n",
        "with open('results.data', 'wb') as filehandle:\n",
        "    pickle.dump(results['boxes'], filehandle)\n",
        "\n",
        "print('Done saving!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggdow2tKAACF"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('results.data')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}